<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="UTF-8">
    <title>Zzr blog</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <!-- build:css /public/css/combined.css-->
    <link rel="stylesheet" href="/public/css/normalize.css" type="text/css">
    <link rel="stylesheet" href="/public/css/main.css" type="text/css">
    <link rel="stylesheet" href="/public/css/unit.css" type="text/css">
    <link rel="stylesheet" href="/public/css/markdown.css" type="text/css">
    <!-- endbuild-->
  </head>
  <body>
    <!--if lte IE 8
    p.browser-upgrade
      | &#x60A8;&#x7684;&#x6D4F;&#x89C8;&#x5668;&#x7248;&#x672C;&#x90FD;&#x8001;&#x7684;&#x6389;&#x7259;&#x5566;&#xFF0C;&#x8BF7;&#x5230;
      a(href='http://browsehappy.com') &#x8FD9;&#x91CC;
      | &#x66F4;&#x65B0;&#xFF0C;&#x4EE5;&#x83B7;&#x53D6;&#x6700;&#x4F73;&#x4F53;&#x9A8C;
    -->
    <section class="sidebar">
      <div class="header"><img src="/public/img/sidebar-brand.jpg" alt="brand" class="header-brand">
        <h1 class="header-name">Zzr&#x7684;&#x535A;&#x5BA2;</h1>
      </div>
      <nav class="main">
        <ul>
          <li class="active"><a href="/" title="博客首页"><i class="iconfont">&#xE600;</i><span>home</span></a></li>
          <li><a href="/demos/demo" title="小的demo"><i class="iconfont">&#xE603;</i><span>demo</span></a></li>
          <li><a href="#" title="关于"><i class="iconfont">&#xE612;</i><span>about</span></a></li>
          <li><a href="https://github.com/Zhangzirui/" title="我的github"><i class="iconfont">&#xE611;</i><span>github</span></a></li>
        </ul>
      </nav>
    </section>
    <div class="container">
      <div class="main">
        <article class="markdown"></p>

<h2>梯度下降算法</h2>

<h3>理解</h3>

<p>线性回归属于监督学习的机器学习算法。给定一些训练数据，为了找到这些数据的规律，就需要通过这些数据来拟合出一条直线，同时也可以对给出的新数据进行预测。</p>

<blockquote><p>markdown不好编辑公式,没办法，所有的公式只能粘图了</p></blockquote>

<p>线性回归模型可以用公式表示为：</p>

<p><img alt="" src="http://cezrh.img48.wal8.com/img48/544629_20160502104557/146381710162.png"/></p>

<p>其中n是变量的个数</p>

<p>则回归模型与真实值之间的误差损失可以表示为</p>

<p><img alt="" src="http://cezrh.img48.wal8.com/img48/544629_20160502104557/146381710222.png"/></p>

<p>其中m是样本的个数</p>

<p>为了让这个回归模型更加可靠真实，就需要找出theta的值使误差损失最小，这样就有了梯度下降算法(gradient descent algorithm)。</p>

<h3>批梯度下降算法(batch gradient descent)</h3>

<p>为了找出合适的theta的值使误差损失最小，首先就要初始化theta，这个初始化是随机的，定它为0还是为1都行。然后通过下面的公式，反复迭代更新theta，直至找出合适的theta。</p>

<p><img alt="" src="http://cezrh.img48.wal8.com/img48/544629_20160502104557/146381889746.png"/></p>

<p>其中alpha为迭代的速度，上式化简可得</p>

<p><img alt="" src="http://cezrh.img48.wal8.com/img48/544629_20160502104557/146381917252.png"/></p>

<p>然后&lt;span id=&#39;jump&#39;&gt;梯度下降算法的核心&lt;/span&gt;就出现啦（具体的说是批梯度下降算法），</p>

<p><img alt="" src="http://cezrh.img48.wal8.com/img48/544629_20160502104557/146381889877.png"/></p>

<p>上程序(python实现，下同)</p>

<pre><code># -*- coding: utf-8 -*-
&#39;&#39;&#39;
Created on 2016年5月21日

@author: Zhangzirui(411489774@qq.com)
&#39;&#39;&#39;
from numpy import *

class Batch_gradient_descent(object): 
    def batch(self,x_arr,y_arr):

        #MINI_ERROR表示系统允许最小误差
        MINI_ERROR = 0.00001 

        #alpha表示学习速率
        alpha = 0.001

        #j_theta用来装载最后的损失
        j_theta = 0

        theta0 = 1
        theta1 = 1
        theta0_last = 0
        theta1_last = 0

        #num用来装载迭代的次数
        num = 0
        m = len(x_arr)

        while True:
            num = num + 1
            j_diff = [0,0]
            for i in range(m):            
                j_diff[0] += (y_arr[i][0] - theta0 -theta1*x_arr[i][0]) 
                j_diff[1] += (y_arr[i][0] - theta0 -theta1*x_arr[i][0]) * x_arr[i][0]

            theta0_last = theta0
            theta1_last = theta1
            theta0 = theta0 + alpha * j_diff[0]
            theta1 = theta1 + alpha * j_diff[1]

            if abs(theta0-theta0_last)&lt;MINI_ERROR and abs(theta1-theta1_last)&lt;MINI_ERROR:
                for j in range(m):
                    j_theta += (theta0 + theta1*x_arr[j][0] - y_arr[j][0])**2*(0.5/m) 
                break

        print &quot;the times of iteration is %d.&quot; % num
        print &#39;Done: theta0 : %f, theta1 : %f,j_theta: %f.&#39; % (theta0,theta1,j_theta)     

    def loadData(self,fileName):
        arr = []
        fr = open(fileName)
        for line in fr.readlines():
            curline = line.strip().split(&quot; &quot;)            
            curline = map(float,curline)
            arr.append(curline)
        return arr    

if __name__ == &quot;__main__&quot;:
    bat = Batch_gradient_descent()
    fileName1 =  &quot;F:\\ML\\stanford\\exercise data\\ex2Data\\ex2x.dat&quot;
    fileName2 =  &quot;F:\\ML\\stanford\\exercise data\\ex2Data\\ex2y.dat&quot;
    x_arr = bat.loadData(fileName1)
    y_arr = bat.loadData(fileName2)  
    bat.batch(x_arr, y_arr) </code></pre>

<p>其中输入的数据来自于<a href="http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&amp;doc=exercises/ex2/ex2.html">Stanford Exercise 2: Linear Regression</a>，结果图如下：</p>

<p><img alt="" src="http://cezrh.img48.wal8.com/img48/544629_20160502104557/146986987107.png"/></p>

<p>接着我想沿用这个程序试一试多元线性回归，数据来源于<a href="http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&amp;doc=exercises/ex3/ex3.html">Stanford Exercise 3</a>。该教程提出来数据预处理，因为x矩阵中两列的数据相差太大，会严重影响梯度下降算法的效率。</p>

<p>可是我发现即使迭代的次数都快突破天际，结果还是没有收敛，始终也无法满足到达最小误差。这是个坑啊，浪费了我很多时间查代码。后来我才想到原因，是我自己陷入思维误区了，它根本就是不能收敛到这么小的误差，所以不该设置最小的误差来判断是否收敛，应该设定最大迭代的次数，看看对于多元线性回归它的收敛情况。</p>

<p>程序为</p>

<pre><code># -*- coding: utf-8 -*-
&#39;&#39;&#39;
Created on 2016年5月21日

@author: Zhangzirui(411489774@qq.com)
&#39;&#39;&#39;

from numpy import *
import matplotlib.pyplot as plt

class Multiple_batch(object):

    def batch(self,x_arr,y_arr):  

        #最大迭代次数
        MAX_ITERATION = 50
        #alpha表示学习速率
        alpha = [0.001,0.01]
        #用来装载最后的误差损失
        j_all = []

        m = len(x_arr)

        #分别用不同收敛速度alpha
        for a in alpha:

            theta0 = 1
            theta1 = 1
            theta2 = 1
            #num表示迭代的次数
            num = 0
            j_thetas = []
            while num&lt;MAX_ITERATION:

                num = num + 1
                j_theta = 0

                #计算每次迭代后的损失函数大小
                for j in range(m):
                    j_theta += (theta0 + theta1*x_arr[j][0] + theta2*x_arr[j][1] - y_arr[j][0])**2*(0.5/m) 
                j_thetas.append(j_theta)

                j_diff = [0,0,0]
                for i in range(m):            
                    j_diff[0] += (y_arr[i][0] - theta0 -theta1*x_arr[i][0] - theta2*x_arr[i][1]) 
                    j_diff[1] += (y_arr[i][0] - theta0 -theta1*x_arr[i][0] - theta2*x_arr[i][1]) * x_arr[i][0]
                    j_diff[2] += (y_arr[i][0] - theta0 -theta1*x_arr[i][0] - theta2*x_arr[i][1]) * x_arr[i][1]

                theta0 = theta0 + a * j_diff[0]
                theta1 = theta1 + a * j_diff[1]
                theta2 = theta2 + a * j_diff[2]          

            j_all.append(j_thetas)        

        mark = [&#39;-r&#39;,&#39;-g&#39;]
        x = arange(0,MAX_ITERATION,1)
        for i in range(len(mark)):
            plt.plot(x,j_all[i],mark[i],linewidth=3)
        plt.show()

    def loadData(self,fileName):
        arr = []
        fr = open(fileName)
        for line in fr.readlines():
            curline = line.strip().split(&quot;   &quot;)
            curline = map(float,curline)
            arr.append(curline)
        return arr    

if __name__ == &quot;__main__&quot;:

    bat = Multiple_batch()
    fileName1 =  &quot;F:\\ML\\stanford\\exercise data\\ex3Data\\ex3x.dat&quot;
    fileName2 =  &quot;F:\\ML\\stanford\\exercise data\\ex3Data\\ex3y.dat&quot;
    x_arr = bat.loadData(fileName1)

    #数据预处理
    x_arr = (x_arr - mean(x_arr, axis=0))/std(x_arr,axis=0)

    y_arr = bat.loadData(fileName2)  
    bat.batch(x_arr, y_arr)   </code></pre>

<p>结果图为：</p>

<p><img alt="" src="http://cezrh.img48.wal8.com/img48/544629_20160502104557/146986987042.png"/></p>

<h3>随机梯度下降算法(stochastic gradient descent)</h3>

<p>随机梯度下降算法其实是针对批梯度下降算法处理大规模数据计算量过大计算时间过长而做出的小改变。让我们看看随机梯度下降算法执行过程：</p>

<p><img alt="" src="http://cezrh.img48.wal8.com/img48/544629_20160502104557/146987081492.png"/></p>

<p>与<a href="#jump">批梯度下降算法的过程</a>相比,批梯度算法是把所有的样本计算一遍后再进行一次theta迭代。而随机梯度下降算法是每计算一个训练样本就迭代一次，这样theta的迭代次数明显加快。当然这样也导致了随机梯度下降算法的计算精度不如批梯度算法。</p>

<p>随机梯度下降的程序和批梯度的差不多，在这里就不贴出来了，想参考的童鞋可以到<a href="#">这里</a>下载。</p>

<h3>梯度下降算法的规范方程（the normal equations）</h3>

<p>这里个人觉得就是将梯度下降算法用矩阵的形式推导一遍，然后得出了简化形式。推导过程在Andrew Ng的机器学习第二节课中讲的非常明白，<a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf">讲义讲义notes1</a>第10页和第11页也给出了详细的推导。
同样运用<a href="http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&amp;doc=exercises/ex2/ex2.html">Stanford Exercise 2: Linear Regression</a>的数据试着编了程序。
	# -<em>- coding: utf-8 -</em>-
	&#39;&#39;&#39;
	Created on 2016年5月9日</p>

<pre><code>@author: Zhangzirui(411489774@qq.com)
&#39;&#39;&#39;
from numpy import * 
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

class LinearRegression(object):

    def loadData(self,fileName):
        dataset = []
        fr = open(fileName)
        for line in fr.readlines():
            curLine = line.strip()
            fltLine = float(curLine)
            dataset.append(fltLine)
        return transpose(mat(dataset))

    def l_r(self,f1,f2):

		#导入原始数据
        x = self.loadData(f1)
        y = self.loadData(f2)

		#画出数据散点图
        plt.figure(&quot;fig1&quot;)
        plt.plot(x,y,&#39;ob&#39;)
        plt.xlabel(&quot;height&quot;)
        plt.ylabel(&quot;years&quot;)

        m = len(y)
        x1 = ones((m,2))
        for i in range(m):
            x1[i,1] = x[i,0]

        theta = zeros((2,1))
        alpha = 0.07
        MAX_IIR = 1500  #theta最大的迭代次数
        ERROR = 1e-10   #允许的误差

        for i in range(MAX_IIR):
            grad = dot(1.0/m*transpose(x1),dot(x1,theta)-y)  #损失函数 = X&#39;*X*theta - X&#39;*Y 
            prev_theta = theta
            theta = theta - alpha*grad
            if [j for j in abs(prev_theta - theta) if j&lt;ERROR] == abs(prev_theta - theta):
                break
 		#画出拟合图像
        plt.plot(x,dot(x1,theta),label=&#39;linear regression&#39;)

        # 为了更好理解batch梯度下降所做的事情，接下来是为了画出J(theta)也就是损失与theta之间的关系
        j_vals = zeros((100,100))
        theta0_vals = linspace(-3,3,100)
        theta1_vals = linspace(-1,1,100)        

        for i in range(len(theta0_vals)):
            for j in range(len(theta1_vals)):
                t = array([[theta0_vals[i]],[theta1_vals[j]]])
                j_vals[i,j] = 0.5/m*sum([val**2 for val in dot(x1,t)-y])   

        fig2 = plt.figure(&quot;fig2&quot;)
        ax = Axes3D(fig2)
        ax.plot_surface(theta0_vals,theta1_vals,j_vals,rstride=5,cstride=5,cmap=&#39;hot&#39;)
		ax.set_xlabel(&#39;theta0&#39;)
    	ax.set_ylabel(&#39;theta1&#39;)
    	ax.set_zlabel(&#39;j(theta)&#39;)
        plt.show()

if __name__ == &quot;__main__&quot;:

    f1 = &quot;F:\ML\stanford\exercise data\ex2Data\ex2x.dat&quot;
    f2 = &quot;F:\ML\stanford\exercise data\ex2Data\ex2y.dat&quot;
    m = LinearRegression()
    m.l_r(f1, f2)</code></pre>

<p>程序图如下：</p>

<p><img alt="" src="http://cezrh.img48.wal8.com/img48/544629_20160502104557/146994974665.png"/>
<img alt="" src="http://cezrh.img48.wal8.com/img48/544629_20160502104557/146995029106.png"/></p>

<p>(完)</p>
          <div class="pay">
            <div class="pay-content">
              <p>如果这篇文章对你很有帮助，你可以犒劳一下WO</p><a class="pay-btn">打赏</a>
            </div>
          </div>
        </article>
      </div>
      <div class="elevator">
        <ul>
          <li id="goTop" title="返回顶部"><i class="iconfont">&#xE605;</i></li>
        </ul>
      </div>
      <footer>
        <p class="copyright">Copyright &copy; Zhangzirui</p>
      </footer>
    </div>
    <script type="text/javascript" src="/public/js/require.js" data-main="/public/js/main.js"></script>
  </body>
</html>